#Ejecuta el Script:
#-------------------------------------------------------------
#Abre una terminal o l√≠nea de comandos, navega al directorio del proyecto y ejecuta:
#python3 03ground_truth.py

#Puedes tambi√©n usar los argumentos si tus carpetas se llaman diferente:
#python3 03ground_truth.py --validation_dir mi_carpeta_de_validacion --output_dir mi_carpeta_de_metricas

#-------------------------------------------------------------
#¬øQu√© hace el script?

#Encuentra el archivo de validaci√≥n: Busca en data_validation/ (o el directorio especificado) el archivo *_muestras_para_validacion.csv m√°s reciente.
#Lee los datos validados: Carga el archivo y se asegura de que existan las columnas GENERO (predicci√≥n de tu script original) y GENERO_VALIDADO (tu entrada manual).
#Filtra validados: Solo considera las filas donde GENERO_VALIDADO tenga un valor (es decir, las que realmente validaste).
#Calcula M√©tricas (usando scikit-learn):
#Accuracy: El porcentaje total de predicciones correctas.
#Classification Report: Proporciona precisi√≥n, recall, F1-score y "support" (n√∫mero de ocurrencias reales) para cada clase ('masculino', 'femenino', 'desconocido', y cualquier otra que hayas usado en GENERO_VALIDADO).
#Precisi√≥n (por clase): De los que el sistema dijo que eran (ej. masculinos), cu√°ntos realmente lo eran. (TP / (TP + FP))
#Recall (por clase): De los que realmente eran (ej. masculinos), cu√°ntos identific√≥ correctamente el sistema. (TP / (TP + FN))
#F1-score (por clase): Media arm√≥nica de precisi√≥n y recall, buen indicador general del rendimiento por clase.
#Confusion Matrix (Matriz de Confusi√≥n): Una tabla que muestra:
#Filas: G√©nero real (de GENERO_VALIDADO).
#Columnas: G√©nero predicho (de GENERO).
#Celdas: N√∫mero de veces que una instancia de un g√©nero real fue clasificada como un g√©nero predicho. Idealmente, los n√∫meros m√°s altos estar√°n en la diagonal (predicciones correctas).
#Muestra en Consola: Imprime las m√©tricas en la terminal.
#Guarda en Archivo:
#Crea la carpeta 03_ground_truth/ si no existe.
#Guarda un archivo CSV (ej. 20231027103000_ground_truth_metrics.csv) que contiene:
#Informaci√≥n general (archivo fuente, fecha, total validados, accuracy).
#El reporte de clasificaci√≥n detallado.
#La matriz de confusi√≥n.
#-------------------------------------------------------------

#-------------------------------------------------------------
#Interpretaci√≥n de las M√©tricas:
#Accuracy alta: Bueno, pero puede ser enga√±oso si las clases est√°n desbalanceadas (ej., si el 90% de tus nombres son masculinos y tu sistema siempre dice "masculino", tendr√° un 90% de accuracy pero ser√° in√∫til para los femeninos).
#Classification Report:
#Clase "desconocido":
#Si el recall para "masculino" y "femenino" es bajo, significa que tu sistema est√° fallando en identificar muchos nombres que s√≠ tienen un g√©nero definido, mand√°ndolos a "desconocido".
#Si la precision para "desconocido" (en y_pred) es baja cuando lo comparas con lo que validaste (quiz√°s algunos "desconocidos" s√≠ ten√≠an g√©nero en tu validaci√≥n), indica que tu sistema est√° siendo demasiado conservador.
#Clases "masculino" / "femenino":
#Busca un buen balance entre precision y recall (reflejado en el F1-score).
#Si la precision es baja para "masculino", significa que cuando tu sistema dice "masculino", a veces se equivoca (es un nombre femenino o desconocido).
#Si el recall es bajo para "masculino", significa que hay muchos nombres masculinos que tu sistema no est√° identificando como tal.
#Matriz de Confusi√≥n: Te muestra exactamente d√≥nde se est√°n cometiendo los errores. Por ejemplo, ¬øcu√°ntos nombres femeninos reales (GENERO_VALIDADO='femenino') fueron incorrectamente clasificados como masculinos (GENERO='masculino')?
#-------------------------------------------------------------


#!pip install scikit-learn 
import pandas as pd
import os
import argparse
from datetime import datetime
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

def encontrar_archivo_validacion_mas_reciente(directorio_data_validation):
    """
    Encuentra el archivo de validaci√≥n m√°s reciente en la carpeta especificada
    basado en el sufijo '_muestras_para_validacion.csv'.
    """
    archivos_candidatos = []
    if not os.path.exists(directorio_data_validation):
        print(f"‚ùå Error: El directorio '{directorio_data_validation}' no existe.")
        return None

    for nombre_archivo in os.listdir(directorio_data_validation):
        if nombre_archivo.endswith('_muestras_para_validacion.csv'):
            archivos_candidatos.append(os.path.join(directorio_data_validation, nombre_archivo))

    if not archivos_candidatos:
        print(f"‚ùå Error: No se encontraron archivos '*_muestras_para_validacion.csv' en '{directorio_data_validation}'.")
        print("   Aseg√∫rate de haber ejecutado primero el script 'generar_muestras_validacion.py' y completado la validaci√≥n manual.")
        return None

    # Ordenar por fecha de modificaci√≥n para obtener el m√°s reciente
    # o por nombre si el nombre contiene la fecha al inicio
    # Asumimos que el nombre contiene la fecha y hora, por lo que el sort alfab√©tico inverso funciona
    archivos_candidatos.sort(key=os.path.basename, reverse=True)
    return archivos_candidatos[0]

def calcular_y_guardar_metricas(archivo_validacion, directorio_salida_metricas):
    """
    Lee el archivo de validaci√≥n completado, calcula m√©tricas y guarda los resultados.
    """
    try:
        df_val = pd.read_csv(archivo_validacion)
        print(f"‚úÖ Archivo de validaci√≥n '{archivo_validacion}' le√≠do correctamente.")
    except FileNotFoundError:
        print(f"‚ùå Error: No se pudo encontrar el archivo de validaci√≥n '{archivo_validacion}'.")
        return
    except Exception as e:
        print(f"‚ùå Error al leer el archivo CSV '{archivo_validacion}': {e}")
        return

    # Verificar columnas necesarias
    columnas_necesarias = ['GENERO', 'GENERO_VALIDADO']
    for col in columnas_necesarias:
        if col not in df_val.columns:
            print(f"‚ùå Error: La columna requerida '{col}' no se encuentra en el archivo de validaci√≥n.")
            print("   Aseg√∫rate de que el archivo de validaci√≥n contiene las columnas 'GENERO' (predicci√≥n) y 'GENERO_VALIDADO' (manual).")
            return

    # Filtrar filas donde GENERO_VALIDADO no est√© vac√≠o (es decir, que fueron validadas)
    df_val_completado = df_val.dropna(subset=['GENERO_VALIDADO'])
    if df_val_completado.empty:
        print("‚ùå Error: No hay filas con 'GENERO_VALIDADO' completado en el archivo.")
        print("   Por favor, completa la validaci√≥n manual en la columna 'GENERO_VALIDADO'.")
        return
    
    print(f"‚ÑπÔ∏è {len(df_val_completado)} registros validados encontrados para el c√°lculo de m√©tricas.")

    y_true = df_val_completado['GENERO_VALIDADO']
    y_pred = df_val_completado['GENERO']

    # Obtener todas las etiquetas √∫nicas presentes en y_true y y_pred para la matriz de confusi√≥n
    labels = sorted(list(set(y_true.unique()) | set(y_pred.unique())))


    # Calcular m√©tricas
    accuracy = accuracy_score(y_true, y_pred)
    report_dict = classification_report(y_true, y_pred, output_dict=True, zero_division=0, labels=labels)
    conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)

    print("\n--- M√©tricas de Clasificaci√≥n ---")
    print(f"Accuracy General: {accuracy:.4f}")
    print("\nReporte de Clasificaci√≥n:")
    print(classification_report(y_true, y_pred, zero_division=0, labels=labels))
    print("\nMatriz de Confusi√≥n:")
    # Crear un DataFrame para la matriz de confusi√≥n para mejor visualizaci√≥n
    df_conf_matrix = pd.DataFrame(conf_matrix, index=[f'Actual: {l}' for l in labels], columns=[f'Predicted: {l}' for l in labels])
    print(df_conf_matrix)

    # Preparar datos para guardar en CSV
    df_report = pd.DataFrame(report_dict).transpose()
    
    # Crear directorio de salida si no existe
    if not os.path.exists(directorio_salida_metricas):
        os.makedirs(directorio_salida_metricas)
        print(f"Directorio '{directorio_salida_metricas}' creado.")

    # Guardar reporte y matriz de confusi√≥n
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    nombre_archivo_salida = f"{timestamp}_ground_truth_metrics.csv"
    ruta_archivo_salida = os.path.join(directorio_salida_metricas, nombre_archivo_salida)

    try:
        with open(ruta_archivo_salida, 'w', encoding='utf-8-sig') as f:
            f.write(f"Metricas de Evaluacion para el archivo: {os.path.basename(archivo_validacion)}\n")
            f.write(f"Fecha de generacion de metricas: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total de registros validados: {len(df_val_completado)}\n")
            f.write(f"Accuracy General: {accuracy:.4f}\n\n")
            
            f.write("Reporte de Clasificacion:\n")
            df_report.to_csv(f, index=True)
            f.write("\n\n")
            
            f.write("Matriz de Confusion:\n")
            df_conf_matrix.to_csv(f, index=True)
        
        print(f"\n‚úÖ M√©tricas guardadas en '{ruta_archivo_salida}'.")

    except Exception as e:
        print(f"‚ùå Error al guardar el archivo de m√©tricas '{ruta_archivo_salida}': {e}")

if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser(description="Calcula m√©tricas de clasificaci√≥n a partir de un archivo de validaci√≥n manual.")
        parser.add_argument(
            "--validation_dir",
            type=str,
            default="02data_validation",
            help="Directorio donde se encuentra el archivo de validaci√≥n completado (por defecto: 02data_validation)."
        )
        parser.add_argument(
            "--output_dir",
            type=str,
            default="03ground_truth",
            help="Directorio donde se guardar√°n las m√©tricas calculadas (por defecto: 03ground_truth)."
        )

        args = parser.parse_args()

        archivo_validacion_seleccionado = encontrar_archivo_validacion_mas_reciente(args.validation_dir)

        if archivo_validacion_seleccionado:
            print(f"‚ÑπÔ∏è Usando el archivo de validaci√≥n m√°s reciente: '{archivo_validacion_seleccionado}'")
            calcular_y_guardar_metricas(archivo_validacion_seleccionado, args.output_dir)
        else:
            print("üö´ No se pudo proceder sin un archivo de validaci√≥n.")

       
    except Exception as error:
        print(f"‚ùå Error inesperado: {error}")
    finally:
        print("üîÑ Proceso de c√°lculo de m√©tricas terminado.")

